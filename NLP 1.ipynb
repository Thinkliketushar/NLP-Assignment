{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f75326ea",
   "metadata": {},
   "source": [
    "# 1. Explain One-Hot Encoding\n",
    "One hot encoding can be defined as the essential process of converting the categorical data variables to be provided to machine and deep learning algorithms which in turn improve predictions as well as classification accuracy of a model.\n",
    "\n",
    "basically we convert the category of column into new columns with 1 else 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1bdb59",
   "metadata": {},
   "source": [
    "# 2. Explain Bag of Words\n",
    "Construct a dictionary of all unique words in the reviews;\n",
    "- d – Unique words: for n reviews or n documents;\n",
    "-Construct a vector of size d; each element in the vector belongs to words such as ‘a’, ‘an’, ‘the’, ‘pasta’, …, ‘tasty’,...\n",
    "- Most of the elements in the vector are zero; thus we will have a sparse matrix;\n",
    "- BOW: The reviews R1 and R2 are completely opposite but the data points are closer to each other; BOW does not perform well when there is a small change in the data;\n",
    "- We can have a binary bag of words;\n",
    "- BOW depend on count of words in each review, this discards the semantic meaning of the documents and documents that are completely opposite in meaning can lie closer to each other when there is very small change in the document (with respect to the words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c9c6bd",
   "metadata": {},
   "source": [
    "# 3. Explain Bag of N-Grams\n",
    "- Uni grams consider each single word for counting; Bi grams considers two consecutive words at a time; We can have n grams similarly;\n",
    "- Uni grams based BOW discards sequence of information; with n grams we can retain some of the sequence information;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dc338a",
   "metadata": {},
   "source": [
    "# 4. Explain TF-IDF\n",
    "TF-IDF (TERM FREQUENCY - INVERSE DOCUMENT FREQUENCY)\n",
    "- TF: Term frequency\n",
    "TF(Wi, rj) = # of times occurs in rj / total number of words in rj; can also be thought of as probability of finding a word Wi in a document rj\n",
    "- IDF(Wi, Dc) = log(N/ni)\n",
    "rj: jth review, Dc: Document corpus, Wi: ith word; N: #of documents, ni = # docs which contain Wi\n",
    "- N/ni >=1; log(N/ni) >= 0 ; IDF >= 0\n",
    "- If ni increases log(N/ni) decreases\n",
    "- If Wi is more frequent in corpus then its IDF is less;\n",
    "- Rare words in documents that occur more frequently in a review  will have high TF-IDF value; TFIDF = tf(Wi, rj) * idf(Wi, Dc)\n",
    "- More importance to rarer words in corpus and give more importance to words that are frequent in a review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d0d5b",
   "metadata": {},
   "source": [
    "# 5. What is OOV problem?\n",
    "Out-of-vocabulary (OOV) are terms that are not part of the normal lexicon found in a natural language processing environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8fedb4",
   "metadata": {},
   "source": [
    "# 6. What are word embeddings?\n",
    " words that are close in meaning are grouped near to one another in vector space like king with queen, man with women, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170a1cfd",
   "metadata": {},
   "source": [
    "# 7. Explain Continuous bag of words (CBOW)\n",
    "CBOW: Continuous Bag of Words:- We have a dictionary/vocab; Use one hot encoding for each word based on vocab length. Core idea behind CBOW: Given context words can we predict focus words. Linear or Identity activation is used. Softmax is used at the output layer. \n",
    "- Take all of the text dataset, create focus word - context words dataset, train the neural network with above structure on this dataset. \n",
    "- CBOW: Predict focus word given context words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47396898",
   "metadata": {},
   "source": [
    "# 8. Explain SkipGram\n",
    "Skip-gram: Predict context words given focus word\n",
    "- Input: v-dimensional one hot encoded focus word. Hidden layer with N dimension with linear activations. Use multi output Softmax layers which gives context words as output corresponding to each output layer that are stacked to form a single output layer (k softmax layers). \n",
    "- Skipgrams: (K+1) (NxV), k softmax: slow training: performs well for less occuring words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65feeca",
   "metadata": {},
   "source": [
    "# 9. Explain Glove Embeddings.\n",
    "It is an unsupervised learning algorithm developed by researchers at Stanford University aiming to generate word embeddings by aggregating global word co-occurrence matrices from a given corpus. The basic idea behind the GloVe word embedding is to derive the relationship between the words from statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deeca8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
